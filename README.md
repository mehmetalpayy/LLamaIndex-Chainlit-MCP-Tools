<h1 align="center"><strong>LLamaIndex-Chainlit-MCP-Tools <strong></h1>

A powerful chatbot application built with Chainlit and LlamaIndex, featuring multiple LLM integrations, conversation history management, and various tools.

## Features

- ğŸ¤– Multiple LLM Support
  - Google Gemini
  - OpenAI GPT (ready to integrate)
  - Anthropic Claude (ready to integrate)

- ğŸ› ï¸ Built-in Tools
  - Mathematical Operations (sum, average, square root)
  - PDF File Analysis
  - More tools can be easily added

- ğŸ“ Advanced Logging
  - Rich Console Output
  - File-based Logging
  - Daily Log Rotation

- ğŸ’¾ Conversation History
  - Session-based Storage
  - JSON Format
  - Persistent Storage in Local Files

- âš™ï¸ Customizable Settings
  - LLM Model Selection
  - Temperature Control
  - Greeting Message Toggle

## Installation

1. Clone the repository:
```bash
git clone https://github.com/mehmetalpayy/LLamaIndex-Chainlit-MCP-Tools.git
cd LLamaIndex-Chainlit-MCP-Tools
```

2. Install uv (if not already installed):
```bash
# Windows (PowerShell)
python -m pip install uv

# Linux/MacOS
python -m pip install uv
```

3. Create and activate virtual environment, then install dependencies:
```bash
# Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/MacOS

# Install dependencies using uv
uv pip install .
```

4. Create `.env` file and add your API key:
```bash
# Create .env file
echo "GEMINI_API_KEY=your_api_key_here" > .env

# Or manually create .env file with:
# GEMINI_API_KEY=your_api_key_here
```

<h2>ğŸ“ Project Structure</h2>

<pre>
LLamaIndex-Chainlit-MCP-Tools/
â”œâ”€â”€ llms/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ gemini.py
â”‚   â”œâ”€â”€ openai.py
â”‚   â””â”€â”€ claude.py
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ math.py
â”‚
â”œâ”€â”€ logger/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logging.py
â”‚   â””â”€â”€ rich_logger.py
â”‚
â”œâ”€â”€ history/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ local_memory.py
â”‚   â””â”€â”€ sessions/
â”‚       â””â”€â”€ {session_id}/
â”‚           â””â”€â”€ messages.json
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ YYYY-MM-DD.log
â”‚
â””â”€â”€ main.py
</pre>


## Usage

1. Start the application:
```bash
chainlit run main.py
```

2. Access the web interface at `http://localhost:8000`

3. Features:
   - Chat with the AI assistant
   - Upload and analyze PDF files
   - Use mathematical tools
   - Customize settings through the UI

## Configuration

### LLM Settings
- Model: Choose between different Gemini models
- Temperature: Adjust response creativity (0.0 - 1.0)
- Greeting: Toggle welcome message

### Logging
- Console: Rich formatted output
- Files: Daily log files in `logs/` directory

### History
- Sessions: Unique ID for each chat session
- Storage: JSON files in `history/sessions/` directory

## Development

### Adding New Tools
1. Create a new file in `tools/`
2. Define your function with `@cl.step(type="tool")`
3. Add to tool list in `main.py`

### Adding New LLMs
1. Create new file in `llms/`
2. Implement using `base.LLMConfig`
3. Add to LLM selection in settings

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request